import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
plt.rcParams['figure.figsize'] = (25.0, 15.0)
import seaborn as sns

url='https://raw.githubusercontent.com/madhavankv/simple-prediction-decision-tree/master/train.csv'

train=pd.read_csv(url)
train.columns
#to select one column
y=train.SalePrice

X=train.drop(['SalePrice','Id'], axis=1)
X.columns=[col.replace(' ', '_').lower() for col in X.columns]
X.head()
plt.rcParams['figure.figsize'] = (25.0, 15.0)
corr=train.corr()
sns.heatmap(corr,annot=True)
#missing value counts in each of these columns
miss = X.isnull().sum()/len(train)
miss = miss[miss > 0]
miss.sort_values(inplace=True)
miss

X=X.drop(['poolqc','miscfeature','alley','fence','fireplacequ'], axis=1)
X.shape
X.describe()

from sklearn.model_selection import train_test_split 
train_X, val_X, train_y, val_y = train_test_split(X,y,random_state=1)
plt.rcParams['figure.figsize'] = (8.0, 6.0)
sns.countplot(train_X['overallqual'])
from sklearn.metrics import mean_absolute_error

#  XGBoost model
from numpy import loadtxt
from xgboost import XGBRegressor
def score_dataset(train_X, val_X, train_y, val_y):
    my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)
    my_model.fit(train_X,train_y,early_stopping_rounds=5,eval_set=[(val_X, val_y)], verbose=False)
    preds = my_model.predict(val_X)
    return mean_absolute_error(val_y, preds)

cols_with_missing = [col for col in train_X.columns if train_X[col].isnull().any()] 
train_X.drop(cols_with_missing, axis=1, inplace=True)
val_X.drop(cols_with_missing, axis=1, inplace=True)

low_cardinality_cols = [cname for cname in train_X.columns if train_X[cname].nunique() < 10 and train_X[cname].dtype == "object"]

# Select numerical columns# Select numerical columns
numerical_cols = [cname for cname in train_X.columns if train_X[cname].dtype in ['int64', 'float64']]

my_cols = low_cardinality_cols + numerical_cols

train_X = train_X[my_cols].copy()
val_X = val_X[my_cols].copy()
# Get list of categorical variables
s = (train_X.dtypes == 'object')
object_cols = list(s[s].index)

print("Categorical variables:")
print(object_cols)
from sklearn.preprocessing import OneHotEncoder

# Apply one-hot encoder to each column with categorical data
OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_X[object_cols]))
OH_cols_valid = pd.DataFrame(OH_encoder.transform(val_X[object_cols]))

# One-hot encoding removed index; put it back
OH_cols_train.index = train_X.index
OH_cols_valid.index = val_X.index

# Remove categorical columns (will replace with one-hot encoding)
num_X_train = train_X.drop(object_cols, axis=1)
num_X_valid = val_X.drop(object_cols, axis=1)

# Add one-hot encoded columns to numerical features
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)



print("MAE from Approach 3 (One-Hot Encoding):") 
print(score_dataset(OH_X_train, OH_X_valid, train_y, val_y))

#create numeric plots
num = [f for f in train.columns if train.dtypes[f] != 'object']
num.remove('Id')
nd = pd.melt(train, value_vars = num)
n1 = sns.FacetGrid (nd, col='variable', col_wrap=4, sharex=False, sharey = False)
n1 = n1.map(sns.distplot, 'value')
n1
